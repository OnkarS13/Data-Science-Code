{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oe9BPh04En0Y"
   },
   "source": [
    "## <Font color=Red>360DigiTMG Assignments<font/>  - <font color=Blue>NaiveBayes<font/>\n",
    "\n",
    "- Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.\n",
    "- It is mainly used in text classification that includes a high-dimensional training dataset.\n",
    "- Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.\n",
    "- It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.\n",
    "Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.\n",
    "    \n",
    "\n",
    "#### The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as:\n",
    "\n",
    "#### Naïve:\n",
    "\n",
    "It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n",
    "#### Bayes:\n",
    "    \n",
    "It is called Bayes because it depends on the principle of Bayes' Theorem\n",
    "\n",
    "\n",
    "#### <font color=Green>Bayes' Theorem:<font/>\n",
    "\n",
    "Bayes' theorem is also known as Bayes' Rule or Bayes' law, which is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability.\n",
    "    \n",
    "The formula for Bayes' theorem is given as:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Where,\n",
    "\n",
    "#### P(A|B) is Posterior probability: \n",
    "    Probability of hypothesis A on the observed event B.\n",
    "\n",
    "#### P(B|A) is Likelihood probability: \n",
    "    Probability of the evidence given that the probability of a hypothesis is true.\n",
    "\n",
    "#### P(A) is Prior Probability: \n",
    "    Probability of hypothesis before observing the evidence.\n",
    "\n",
    "#### P(B) is Marginal Probability: \n",
    "    Probability of Evidence.\n",
    "    \n",
    "    \n",
    "#### <font color=purple>Advantages of Naïve Bayes Classifier:<font/>\n",
    "    \n",
    "- Naïve Bayes is one of the fast and easy ML algorithms to predict a class of datasets.\n",
    "- It can be used for Binary as well as Multi-class Classifications.\n",
    "- It performs well in Multi-class predictions as compared to the other Algorithms.\n",
    "- It is the most popular choice for text classification problems.\n",
    "\n",
    "#### <font color=purple>Disadvantages of Naïve Bayes Classifier:<font/>\n",
    "    \n",
    "- Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features.\n",
    "\n",
    "#### <font color=purple>Applications of Naïve Bayes Classifier:<font/>\n",
    "    \n",
    "- It is used for Credit Scoring.\n",
    "- It is used in medical data classification.\n",
    "- It can be used in real-time predictions because Naïve Bayes Classifier is an eager learner.\n",
    "- It is used in Text classification such as Spam filtering and Sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dHZK7R1En0c"
   },
   "source": [
    "## Problem 2). <font color=darkblue >Build a Naive Bayes model on the data set for classifying if the Business Social network user is going to buy luxury SUV or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HskU2u_1En0d"
   },
   "source": [
    "## <font color=blue>Step 1.<font/>\n",
    "\n",
    "### <font color=red>Bussiness objective:-<font/> <font color=black>To classify whether a Business social network user is going to buy luxury SUV or not .<font/>\n",
    "\n",
    "### <font color=red>Bussiness contraints:- <font/> <font color=black>Minimize the model error percentage.<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRim_ItREn0d"
   },
   "source": [
    "## <font color=blue>Step 2.<font/>\n",
    "\n",
    "### <font color=red>Data Understanding and Analysis<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uY3d-8waEn0e"
   },
   "outputs": [],
   "source": [
    "# Importig all the dependencies/lobraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tlhDnj42En0f"
   },
   "outputs": [],
   "source": [
    "######## Loading the data set\n",
    "cars = pd.read_csv(\"NB_Car_Ad.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-j6pmQvEn0f"
   },
   "source": [
    "## <font color=blue>Step 3.<font/>\n",
    "\n",
    "### <font color=red>Data Pre-Cleansing or Data Preparation:<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gb75_xRmEn0f"
   },
   "outputs": [],
   "source": [
    "# dropping first column\n",
    "car_data = cars.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Lcqwi9uEn0g"
   },
   "source": [
    "#### Label Encoding Gender column to change from categorical to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KjW_vxdfEn0g"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "car_data['Gender'] = le.fit_transform(car_data['Gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkrURBowEn0g"
   },
   "source": [
    "### Splitting data into train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VglKUuw4En0h"
   },
   "outputs": [],
   "source": [
    "X = car_data.iloc[:, :-1]\n",
    "y = car_data.iloc[:, -1]\n",
    "\n",
    "# splitting data into train and test data sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y7-K8KhAEn0h"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePXmP4BIEn0i"
   },
   "source": [
    "## <font color=blue>Step 4.<font/>\n",
    "\n",
    "### <font color=red>Model Building - Bernoulli NaiveBayes<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9nsjaGCHEn0i",
    "outputId": "a1c8ae70-c8e9-4abf-d61c-070eefa0e245"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preparing a naive bayes model on training data set \n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z98gaLugEn0j"
   },
   "source": [
    "## <font color=blue>Step 5.<font/>\n",
    "\n",
    "### <font color=red>Model Evaluation - Bernoulli Naive Bayes<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred  =  classifier.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "ac = accuracy_score(y_test,y_pred)\n",
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  3],\n",
       "       [14, 14]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Q 2- 360DigiTMG Assignments - NaiveBayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
