{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oe9BPh04En0Y"
   },
   "source": [
    "## <Font color=Red>360DigiTMG Assignments<font/>  - <font color=Blue>NaiveBayes<font/>\n",
    "\n",
    "- Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.\n",
    "- It is mainly used in text classification that includes a high-dimensional training dataset.\n",
    "- Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions.\n",
    "- It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.\n",
    "Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.\n",
    "    \n",
    "\n",
    "#### The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as:\n",
    "\n",
    "#### Naïve:\n",
    "\n",
    "It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n",
    "#### Bayes:\n",
    "    \n",
    "It is called Bayes because it depends on the principle of Bayes' Theorem\n",
    "\n",
    "\n",
    "#### <font color=Green>Bayes' Theorem:<font/>\n",
    "\n",
    "Bayes' theorem is also known as Bayes' Rule or Bayes' law, which is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability.\n",
    "    \n",
    "The formula for Bayes' theorem is given as:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Where,\n",
    "\n",
    "#### P(A|B) is Posterior probability: \n",
    "    Probability of hypothesis A on the observed event B.\n",
    "\n",
    "#### P(B|A) is Likelihood probability: \n",
    "    Probability of the evidence given that the probability of a hypothesis is true.\n",
    "\n",
    "#### P(A) is Prior Probability: \n",
    "    Probability of hypothesis before observing the evidence.\n",
    "\n",
    "#### P(B) is Marginal Probability: \n",
    "    Probability of Evidence.\n",
    "    \n",
    "    \n",
    "#### <font color=purple>Advantages of Naïve Bayes Classifier:<font/>\n",
    "    \n",
    "- Naïve Bayes is one of the fast and easy ML algorithms to predict a class of datasets.\n",
    "- It can be used for Binary as well as Multi-class Classifications.\n",
    "- It performs well in Multi-class predictions as compared to the other Algorithms.\n",
    "- It is the most popular choice for text classification problems.\n",
    "\n",
    "#### <font color=purple>Disadvantages of Naïve Bayes Classifier:<font/>\n",
    "    \n",
    "- Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features.\n",
    "\n",
    "#### <font color=purple>Applications of Naïve Bayes Classifier:<font/>\n",
    "    \n",
    "- It is used for Credit Scoring.\n",
    "- It is used in medical data classification.\n",
    "- It can be used in real-time predictions because Naïve Bayes Classifier is an eager learner.\n",
    "- It is used in Text classification such as Spam filtering and Sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dHZK7R1En0c"
   },
   "source": [
    "## Problem 3). <font color=darkblue >Build a Naive Bayes model on the data set for classifying if the Business Social network user is going to buy luxury SUV or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HskU2u_1En0d"
   },
   "source": [
    "## <font color=blue>Step 1.<font/>\n",
    "\n",
    "### <font color=red>Bussiness objective:-<font/> <font color=black>To classify whether a given Tweet about a real disaster  is Real or Fake.<font/>\n",
    "\n",
    "### <font color=red>Bussiness contraints:- <font/> <font color=black>Minimize the model error percentage.<font/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRim_ItREn0d"
   },
   "source": [
    "## <font color=blue>Step 2.<font/>\n",
    "\n",
    "### <font color=red>Data Understanding and Analysis<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uY3d-8waEn0e"
   },
   "outputs": [],
   "source": [
    "# Importig all the dependencies/lobraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tlhDnj42En0f"
   },
   "outputs": [],
   "source": [
    "######## Loading the data set\n",
    "tweets = pd.read_csv(\"Disaster_tweets_NB.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-j6pmQvEn0f"
   },
   "source": [
    "## <font color=blue>Step 3.<font/>\n",
    "\n",
    "### <font color=red>Data Pre-Cleansing or Data Preparation:<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = tweets.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gb75_xRmEn0f"
   },
   "outputs": [],
   "source": [
    "# cleaning data \n",
    "import re\n",
    "stop_words = []\n",
    "\n",
    "# Load the custom built Stopwords\n",
    "with open(\"stopwords_en.txt\",\"r\") as sw:\n",
    "    stop_words = sw.read()\n",
    "\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "   \n",
    "def cleaning_text(i):\n",
    "    i = re.sub(\"[^A-Za-z\" \"]+\",\" \",i).lower()\n",
    "    i = re.sub(\"[0-9\" \"]+\",\" \",i)\n",
    "    w = []\n",
    "    for word in i.split(\" \"):\n",
    "        if len(word)>3:\n",
    "            w.append(word)\n",
    "    return (\" \".join(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Lcqwi9uEn0g"
   },
   "source": [
    "#### Testing the above specified function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KjW_vxdfEn0g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hope having good week just checking\n",
      "hope understand your feelings\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "# testing above function with sample text => removes punctuations, numbers\n",
    "print(cleaning_text(\"Hope you are having a good week. Just checking in\"))\n",
    "print(cleaning_text(\"hope i can understand your feelings 123121. 123 hi how .. are you?\"))\n",
    "print(cleaning_text(\"Hi how are you, I am good\"))\n",
    "\n",
    "tweet_data.text = tweet_data.text.apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty rows\n",
    "tweet_data = tweet_data.loc[tweet_data.text != \" \",:]\n",
    "\n",
    "# CountVectorizer\n",
    "# Convert a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkrURBowEn0g"
   },
   "source": [
    "### splitting data into train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VglKUuw4En0h"
   },
   "outputs": [],
   "source": [
    "# splitting data into train and test data sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweet_train, tweet_test = train_test_split(tweet_data, test_size = 0.3)\n",
    "\n",
    "# creating a matrix of token counts for the entire text document \n",
    "def split_into_words(i):\n",
    "    return [word for word in i.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y7-K8KhAEn0h"
   },
   "outputs": [],
   "source": [
    "# Defining the preparation of tweet texts into word count matrix format - Bag of Words\n",
    "tweets_bow = CountVectorizer(analyzer = split_into_words).fit(tweet_data.text)\n",
    "\n",
    "# Defining BOW for all messages\n",
    "all_tweets_matrix = tweets_bow.transform(tweet_data.text)\n",
    "\n",
    "# For training messages\n",
    "train_tweets_matrix = tweets_bow.transform(tweet_train.text)\n",
    "\n",
    "# For testing messages\n",
    "test_tweets_matrix = tweets_bow.transform(tweet_test.text)\n",
    "\n",
    "# Learning Term weighting and normalizing on entire tweets\n",
    "tfidf_transformer = TfidfTransformer().fit(all_tweets_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "G4XPGEqkEn0h",
    "outputId": "12225cb3-a0f5-4adc-882f-83261b33f5a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 19280)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing TFIDF for train tweets\n",
    "train_tfidf = tfidf_transformer.transform(train_tweets_matrix)\n",
    "train_tfidf.shape # (row, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kdK-LEt9En0i",
    "outputId": "477f6261-7d12-4b6f-fd88-f505332e765f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2284, 19280)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing TFIDF for test tweets\n",
    "test_tfidf = tfidf_transformer.transform(test_tweets_matrix)\n",
    "test_tfidf.shape #  (row, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePXmP4BIEn0i"
   },
   "source": [
    "## <font color=blue>Step 4.<font/>\n",
    "\n",
    "### <font color=red>Model Building - Multinomial NaiveBayes<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9nsjaGCHEn0i",
    "outputId": "a1c8ae70-c8e9-4abf-d61c-070eefa0e245"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing a naive bayes model on training data set \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB as MB\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "classifier_mb = MB()\n",
    "classifier_mb.fit(train_tfidf, tweet_train.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z98gaLugEn0j"
   },
   "source": [
    "## <font color=blue>Step 5.<font/>\n",
    "\n",
    "### <font color=red>Model Evaluation - Multinomial Naive Bayes<font/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3wc7sopZEn0j",
    "outputId": "963e2b0c-d501-4ee8-e6d8-e62f39961f17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044848939763558"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training Data accuracy\n",
    "train_pred_m = classifier_mb.predict(train_tfidf)\n",
    "accuracy_train_m = np.mean(train_pred_m == tweet_train.target)\n",
    "accuracy_train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dFm-69bTEn0j",
    "outputId": "33322a13-0152-4149-b0ea-8e408aa92cc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7898423817863398"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on Test Data\n",
    "test_pred_m = classifier_mb.predict(test_tfidf)\n",
    "accuracy_test_m = np.mean(test_pred_m == tweet_test.target)\n",
    "accuracy_test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "sSJNZ81xEn0j",
    "outputId": "a05ace26-a869-4cde-9e0e-363b4453b77e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1178</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target     0    1\n",
       "row_0            \n",
       "0       1178  390\n",
       "1         90  626"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_pred_m, tweet_test.target) \n",
    "pd.crosstab(test_pred_m, tweet_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes changing default alpha for laplace smoothing\n",
    "# if alpha = 0 then no smoothing is applied and the default alpha parameter is 1\n",
    "# the smoothing process mainly solves the emergence of zero probability problem in the dataset.\n",
    "\n",
    "classifier_mb_lap = MB(alpha = 3)\n",
    "classifier_mb_lap.fit(train_tfidf, tweet_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8556952523925689"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Data accuracy\n",
    "train_pred_lap = classifier_mb_lap.predict(train_tfidf)\n",
    "accuracy_train_lap = np.mean(train_pred_lap == tweet_train.target)\n",
    "accuracy_train_lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666374781085814"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on Test Data after applying laplace\n",
    "test_pred_lap = classifier_mb_lap.predict(test_tfidf)\n",
    "accuracy_test_lap = np.mean(test_pred_lap == tweet_test.target)\n",
    "accuracy_test_lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1214</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target     0    1\n",
       "row_0            \n",
       "0       1214  479\n",
       "1         54  537"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_pred_lap, tweet_test.target) \n",
    "\n",
    "pd.crosstab(test_pred_lap, tweet_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Laplace Smoothining, We got better results.\n",
    "False tweets are getting detected as False more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Q 2- 360DigiTMG Assignments - NaiveBayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
